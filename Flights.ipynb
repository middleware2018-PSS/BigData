{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":24,\"axes.titlesize\":24,\"axes.labelsize\":24})   \n",
    "\n",
    "\n",
    "# import matplotlib and allow it to plot inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn can generate several warnings, we ignore them\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries:\n",
    "- the percentage of canceled flights per day, throughout the entire data set\n",
    "- weekly percentages of delays that are due to weather, throughout the entire data set \n",
    "- the percentage of flights belonging to a given \"distance group\" that were able to halve their departure delays by the time they arrived at their destinations. Distance groups assort flights by their total distance in miles. Flights with distances that are less than 200 miles belong in group 1, flights with distances that are between 200 and 399 miles belong in group 2, flights with distances that are between 400 and 599 miles belong in group 3, and so on. The last group contains flights whose distances are between 2400 and 2599 miles.\n",
    "- a weekly \"penalty\" score for each airport that depends on both the its incoming and outgoing flights. The score adds 0.5 for each incoming flight that is more than 15 minutes late, and 1 for each outgoing flight that is more than 15 minutes late.\n",
    "\n",
    "\n",
    "# \"The percentage of canceled flights per day, throughout the entire data set\"\n",
    "## Sql version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('BDdata/1994.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5180048"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+\n",
      "|Year|Month|DayofMonth|cancelled|\n",
      "+----+-----+----------+---------+\n",
      "|1994|   10|        28|       37|\n",
      "|1994|   12|        26|      159|\n",
      "|1994|    2|        11|     3649|\n",
      "|1994|   12|        10|       80|\n",
      "|1994|    4|        13|      368|\n",
      "|1994|    6|        23|       88|\n",
      "|1994|    9|        26|       59|\n",
      "|1994|   12|        11|       76|\n",
      "|1994|   11|        24|       14|\n",
      "|1994|   12|         4|       80|\n",
      "|1994|    1|        13|      315|\n",
      "|1994|    4|        28|      124|\n",
      "|1994|    4|        24|       22|\n",
      "|1994|    5|        12|       72|\n",
      "|1994|    8|         8|       43|\n",
      "|1994|    4|        29|      107|\n",
      "|1994|    4|        16|       65|\n",
      "|1994|   10|        20|      212|\n",
      "|1994|    1|         8|     1206|\n",
      "|1994|   12|        29|       54|\n",
      "+----+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_ymdc= d.select(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"],d[\"Cancelled\"])\n",
    "d_cancelled = d_ymdc.filter(d[\"Cancelled\"]==1). \\\n",
    "    groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]). \\\n",
    "    count().withColumnRenamed(\"count\",\"cancelled\")\n",
    "d_cancelled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----+\n",
      "|Year|Month|DayofMonth|total|\n",
      "+----+-----+----------+-----+\n",
      "|1994|   10|        28|14847|\n",
      "|1994|   12|        26|14751|\n",
      "|1994|    2|        11|14242|\n",
      "|1994|   12|        10|13409|\n",
      "|1994|    4|        13|14450|\n",
      "|1994|    6|        23|14527|\n",
      "|1994|    9|        26|14680|\n",
      "|1994|   12|        11|14158|\n",
      "|1994|   11|        24|11524|\n",
      "|1994|   12|         4|14088|\n",
      "|1994|    1|        13|14062|\n",
      "|1994|    4|        24|13661|\n",
      "|1994|    4|        28|14364|\n",
      "|1994|    5|        12|14462|\n",
      "|1994|    8|         8|14819|\n",
      "|1994|    4|        29|14226|\n",
      "|1994|    4|        16|12845|\n",
      "|1994|   10|        20|14800|\n",
      "|1994|    1|         8|12605|\n",
      "|1994|   12|        29|14922|\n",
      "+----+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_total = d_ymdc. \\\n",
    "    groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]). \\\n",
    "    count().withColumnRenamed(\"count\",\"total\")\n",
    "d_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-----+\n",
      "|Year|Month|DayofMonth|cancelled|total|\n",
      "+----+-----+----------+---------+-----+\n",
      "|1994|   10|        28|       37|14847|\n",
      "|1994|   12|        26|      159|14751|\n",
      "|1994|    2|        11|     3649|14242|\n",
      "|1994|   12|        10|       80|13409|\n",
      "|1994|    4|        13|      368|14450|\n",
      "|1994|    6|        23|       88|14527|\n",
      "|1994|    9|        26|       59|14680|\n",
      "|1994|   12|        11|       76|14158|\n",
      "|1994|   11|        24|       14|11524|\n",
      "|1994|   12|         4|       80|14088|\n",
      "|1994|    1|        13|      315|14062|\n",
      "|1994|    4|        24|       22|13661|\n",
      "|1994|    4|        28|      124|14364|\n",
      "|1994|    5|        12|       72|14462|\n",
      "|1994|    8|         8|       43|14819|\n",
      "|1994|    4|        29|      107|14226|\n",
      "|1994|    4|        16|       65|12845|\n",
      "|1994|   10|        20|      212|14800|\n",
      "|1994|    1|         8|     1206|12605|\n",
      "|1994|   12|        29|       54|14922|\n",
      "+----+-----+----------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = d_cancelled.join(d_total, on=[d_cancelled.Year == d_total.Year, d_cancelled.Month == d_total.Month,d_cancelled.DayofMonth == d_total.DayofMonth])\\\n",
    "    .select(d_cancelled.Year,d_cancelled.Month,d_cancelled.DayofMonth,d_cancelled[\"cancelled\"],d_total[\"total\"])\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+--------------------+\n",
      "|Year|Month|DayofMonth| percentageCancelled|\n",
      "+----+-----+----------+--------------------+\n",
      "|1994|    1|         1|0.005264023688106...|\n",
      "|1994|    1|         2|0.004492230650268797|\n",
      "|1994|    1|         3| 0.01541819205857505|\n",
      "|1994|    1|         4| 0.15560882746950574|\n",
      "|1994|    1|         5|0.047656139357031585|\n",
      "|1994|    1|         6| 0.05416755640970888|\n",
      "|1994|    1|         7| 0.08754974417282547|\n",
      "|1994|    1|         8| 0.09567631892106307|\n",
      "|1994|    1|         9|0.011112791049289385|\n",
      "|1994|    1|        10|0.018481801444554597|\n",
      "|1994|    1|        11|0.013738403795765172|\n",
      "|1994|    1|        12| 0.07870958633347451|\n",
      "|1994|    1|        13|0.022400796472763475|\n",
      "|1994|    1|        14| 0.01504349066020248|\n",
      "|1994|    1|        15|  0.0120414673046252|\n",
      "|1994|    1|        16|0.047875201721355565|\n",
      "|1994|    1|        17| 0.17112681081852565|\n",
      "|1994|    1|        18| 0.11705949985628054|\n",
      "|1994|    1|        19| 0.07063008130081301|\n",
      "|1994|    1|        20|0.031078742428612634|\n",
      "+----+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.withColumn(\"percentageCancelled\", (res.cancelled/res.total))\\\n",
    "    .drop(\"cancelled\",\"total\")\\\n",
    "    .orderBy(\"Year\",\"Month\",\"DayofMonth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCancelledPercentage(file):\n",
    "    d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "    .load(file)\n",
    "    \n",
    "    d_ymdc= d.select(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"],d[\"Cancelled\"])\n",
    "    \n",
    "    d_cancelled = d_ymdc.filter(d[\"Cancelled\"]==1) \\\n",
    "        .groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]) \\\n",
    "        .count().withColumnRenamed(\"count\",\"cancelled\")\n",
    "    \n",
    "    d_total = d_ymdc \\\n",
    "        .groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]) \\\n",
    "        .count().withColumnRenamed(\"count\",\"total\")\n",
    "    \n",
    "    res = d_cancelled.join(d_total, on=[d_cancelled.Year == d_total.Year, d_cancelled.Month == d_total.Month,d_cancelled.DayofMonth == d_total.DayofMonth]) \\\n",
    "        .select(d_cancelled.Year,d_cancelled.Month,d_cancelled.DayofMonth,d_cancelled[\"cancelled\"],d_total[\"total\"])\n",
    "    \n",
    "    return res.withColumn(\"percentageCancelled\", (res.cancelled/res.total))\\\n",
    "        .drop(\"cancelled\",\"total\")\\\n",
    "        .orderBy(\"Year\",\"Month\",\"DayofMonth\")\n",
    "        \n",
    "\n",
    "res1=[getCancelledPercentage(\"BDdata/\"+str(i)+\".csv\") for i in range(1994,2009)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+--------------------+\n",
      "|Year|Month|DayofMonth| percentageCancelled|\n",
      "+----+-----+----------+--------------------+\n",
      "|1995|    1|         1|0.010582010582010581|\n",
      "|1995|    1|         2|0.010852713178294573|\n",
      "|1995|    1|         3|0.017752621084453593|\n",
      "|1995|    1|         4| 0.02272874023374443|\n",
      "|1995|    1|         5| 0.02155618850336613|\n",
      "|1995|    1|         6| 0.06094487171201448|\n",
      "|1995|    1|         7|  0.0257183908045977|\n",
      "|1995|    1|         8| 0.01808698008399946|\n",
      "|1995|    1|         9|  0.0530588388102351|\n",
      "|1995|    1|        10| 0.04393730736441734|\n",
      "|1995|    1|        11| 0.12992796332678455|\n",
      "|1995|    1|        12| 0.09618030531350324|\n",
      "|1995|    1|        13|  0.0287468966418398|\n",
      "|1995|    1|        14|0.027137140778464897|\n",
      "|1995|    1|        15|0.019543303846945075|\n",
      "|1995|    1|        16|0.020290613954706112|\n",
      "|1995|    1|        17|0.016901408450704224|\n",
      "|1995|    1|        18|0.031135291033036184|\n",
      "|1995|    1|        19| 0.04605004585353072|\n",
      "|1995|    1|        20|0.028532164125384465|\n",
      "+----+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results[1].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sc.textFile('./BDdata/1994.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1994, 1, 5), 0.047656139357031585),\n",
       " ((1994, 2, 1), 0.006602768903088392),\n",
       " ((1994, 2, 2), 0.005858685677984047),\n",
       " ((1994, 2, 3), 0.007247906551263106),\n",
       " ((1994, 5, 18), 0.0035822540644805732),\n",
       " ((1994, 5, 19), 0.002082176568573015),\n",
       " ((1994, 5, 21), 0.003274942878903275),\n",
       " ((1994, 5, 16), 0.005455047645352852),\n",
       " ((1994, 6, 25), 0.009863996413092213),\n",
       " ((1994, 6, 26), 0.003399638336347197),\n",
       " ((1994, 6, 27), 0.0069387194284144),\n",
       " ((1994, 6, 28), 0.003035738926452325),\n",
       " ((1994, 7, 12), 0.008371333287960253),\n",
       " ((1994, 7, 13), 0.007550506768247058),\n",
       " ((1994, 7, 14), 0.017633442265795208),\n",
       " ((1994, 8, 8), 0.0029016802753222214),\n",
       " ((1994, 8, 9), 0.0019498419955624286),\n",
       " ((1994, 8, 11), 0.003987025273685633),\n",
       " ((1994, 8, 14), 0.014527673446247608),\n",
       " ((1994, 11, 23), 0.0013551053594416966)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsplitted = ds.map(lambda line : line.split(\",\"))\n",
    "\n",
    "head = dsplitted.take(1)[0]\n",
    "dsfiltered = dsplitted.filter(lambda x : x != head)\n",
    "\n",
    "dmapped = dsfiltered.map(lambda x : (tuple([int(el) for el in x[0:3]]),int(x[21])))\n",
    "\n",
    "date_cancelled = dmapped.reduceByKey(lambda a, b : a+b)\n",
    "total_per_date = dmapped.map(lambda x: (x[0],1)).reduceByKey(lambda a, b : a+b)\n",
    "\n",
    "results=date_cancelled.join(total_per_date).map(lambda x : (x[0], x[1][0]/x[1][1]))\n",
    "results.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPercentage(year):\n",
    "    ds = sc.textFile('./BDdata/'+year+'.csv')\n",
    "    dsplitted = ds.map(lambda line : line.split(\",\"))\n",
    "\n",
    "    head = dsplitted.take(1)[0]\n",
    "    dsfiltered = dsplitted.filter(lambda x : x != head)\n",
    "\n",
    "    dmapped = dsfiltered.map(lambda x : (tuple([int(el) for el in x[0:3]]),int(x[21])))\n",
    "\n",
    "    date_cancelled = dmapped.reduceByKey(lambda a, b : a+b)\n",
    "    total_per_date = dmapped.map(lambda x: (x[0],1)).reduceByKey(lambda a, b : a+b)\n",
    "\n",
    "    results=date_cancelled.join(total_per_date).map(lambda x : (x[0], x[1][0]/x[1][1]))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = [getPercentage(str(i)).collect() for i in range(1994,2009)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra query: Weekly percentages of flights cancelled that are due to weather, throughout the entire data set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "    .load('BDdata/2003.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter only the flight that have been cancelled and select only the necessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = d.filter(d[\"Cancelled\"]=='1')\\\n",
    "    .select(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"],d[\"CancellationCode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter only the flights that have been cancelled due to the weather conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----------------+\n",
      "|Year|Month|DayofMonth|CancellationCode|\n",
      "+----+-----+----------+----------------+\n",
      "|2003|    6|         1|               B|\n",
      "|2003|    6|        20|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        13|               B|\n",
      "|2003|    6|        25|               B|\n",
      "|2003|    6|        13|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        13|               B|\n",
      "|2003|    6|        27|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        13|               B|\n",
      "|2003|    6|        13|               B|\n",
      "|2003|    6|        27|               B|\n",
      "|2003|    6|        23|               B|\n",
      "|2003|    6|        20|               B|\n",
      "|2003|    6|        12|               B|\n",
      "|2003|    6|        20|               B|\n",
      "+----+-----+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_cancelled_weather = dr.filter(d[\"CancellationCode\"]=='B')\n",
    "d_cancelled_weather.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the total ammount of cancelled flights per day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+\n",
      "|Year|Month|DayofMonth|cancelled|\n",
      "+----+-----+----------+---------+\n",
      "|2003|    1|        19|      184|\n",
      "|2003|    7|        29|      172|\n",
      "|2003|   12|         5|     1274|\n",
      "|2003|   11|         7|      107|\n",
      "|2003|    3|        26|      187|\n",
      "|2003|    7|        17|      286|\n",
      "|2003|   10|        25|      115|\n",
      "|2003|    5|        12|      154|\n",
      "|2003|    6|        30|      137|\n",
      "|2003|    6|        15|       84|\n",
      "|2003|   11|         5|      423|\n",
      "|2003|    1|        22|      342|\n",
      "|2003|    2|         8|       96|\n",
      "|2003|    2|        10|      532|\n",
      "|2003|    2|        19|      218|\n",
      "|2003|    6|         9|      100|\n",
      "|2003|    8|        15|     1118|\n",
      "|2003|    8|         9|       71|\n",
      "|2003|    7|        10|      380|\n",
      "|2003|    5|        19|      125|\n",
      "+----+-----+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d2 = dr.groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]).count().withColumnRenamed(\"count\",\"cancelled\")\n",
    "d2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then notice that no data is available before June 2003:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+\n",
      "|Year|Month|DayofMonth|weather|\n",
      "+----+-----+----------+-------+\n",
      "|2003|    6|         1|      8|\n",
      "|2003|    6|         2|      4|\n",
      "|2003|    6|         3|     15|\n",
      "|2003|    6|         4|     23|\n",
      "|2003|    6|         5|     46|\n",
      "|2003|    6|         6|      5|\n",
      "|2003|    6|         7|      4|\n",
      "|2003|    6|         8|     24|\n",
      "|2003|    6|         9|      8|\n",
      "|2003|    6|        10|      7|\n",
      "|2003|    6|        11|     20|\n",
      "|2003|    6|        12|    191|\n",
      "|2003|    6|        13|    175|\n",
      "|2003|    6|        14|     44|\n",
      "|2003|    6|        15|      5|\n",
      "|2003|    6|        16|     44|\n",
      "|2003|    6|        17|      8|\n",
      "|2003|    6|        18|     38|\n",
      "|2003|    6|        19|     13|\n",
      "|2003|    6|        20|     11|\n",
      "+----+-----+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d3 = d_cancelled_weather.groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"])\\\n",
    "        .count()\\\n",
    "        .withColumnRenamed(\"count\",\"weather\")\n",
    "\n",
    "d3.orderBy(d2[\"Year\"],d2[\"Month\"],d2[\"DayofMonth\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We join the data of the total cancelled flights and the ones cancelled due to weather in a single table, ordering it afterwards to better visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+\n",
      "|Year|Month|DayofMonth|cancelled|weather|\n",
      "+----+-----+----------+---------+-------+\n",
      "|2003|    6|         1|       73|      8|\n",
      "|2003|    6|         2|       94|      4|\n",
      "|2003|    6|         3|      119|     15|\n",
      "|2003|    6|         4|      148|     23|\n",
      "|2003|    6|         5|      178|     46|\n",
      "|2003|    6|         6|      102|      5|\n",
      "|2003|    6|         7|       63|      4|\n",
      "|2003|    6|         8|      119|     24|\n",
      "|2003|    6|         9|      100|      8|\n",
      "|2003|    6|        10|      124|      7|\n",
      "|2003|    6|        11|      143|     20|\n",
      "|2003|    6|        12|      526|    191|\n",
      "|2003|    6|        13|      385|    175|\n",
      "|2003|    6|        14|      143|     44|\n",
      "|2003|    6|        15|       84|      5|\n",
      "|2003|    6|        16|      182|     44|\n",
      "|2003|    6|        17|      129|      8|\n",
      "|2003|    6|        18|      225|     38|\n",
      "|2003|    6|        19|      102|     13|\n",
      "|2003|    6|        20|      102|     11|\n",
      "+----+-----+----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d4 = d2.join(d3, on=[d2[\"Year\"]==d3[\"Year\"],d2[\"Month\"]==d3[\"Month\"],d2[\"DayofMonth\"]==d3[\"DayofMonth\"]])\\\n",
    "    .select(d2[\"Year\"],d2[\"Month\"],d2[\"DayofMonth\"],d2[\"cancelled\"],d3[\"weather\"])\\\n",
    "    .orderBy(d2[\"Year\"],d2[\"Month\"],d2[\"DayofMonth\"])\n",
    "d4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to associate to each row the corresponding week number, taking into account the fact the the last days of December counts as first week of the successive year, so we have to remove them before grouping by week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+--------------+\n",
      "|Year|week|sum(weather)|sum(cancelled)|\n",
      "+----+----+------------+--------------+\n",
      "|2003|  50|        1324|          3212|\n",
      "|2003|  32|         339|          1654|\n",
      "|2003|  35|         462|          1755|\n",
      "|2003|  46|         677|          2119|\n",
      "|2003|  40|         107|           889|\n",
      "|2003|  22|           8|            73|\n",
      "|2003|  49|        3527|          5135|\n",
      "|2003|  25|         119|           877|\n",
      "|2003|  28|         804|          2662|\n",
      "|2003|  26|          79|           746|\n",
      "|2003|  37|         203|          1489|\n",
      "|2003|  51|         326|          1671|\n",
      "|2003|  33|         293|          2942|\n",
      "|2003|  36|         114|          1160|\n",
      "|2003|  27|         361|          1101|\n",
      "|2003|  23|         121|           823|\n",
      "|2003|  47|         952|          2348|\n",
      "|2003|  44|         206|          1547|\n",
      "|2003|  29|         255|          1302|\n",
      "|2003|  41|         190|           760|\n",
      "+----+----+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "week = udf(lambda *x : datetime(x[0],x[1],x[2]).isocalendar()[1], IntegerType())\n",
    "\n",
    "d5 = d4.withColumn(\"week\", week(d4[\"Year\"],d4[\"Month\"],d4[\"DayofMonth\"]))\n",
    "\n",
    "newYearFirstWeek = d5.filter(d5[\"week\"]==1).filter(d5[\"Month\"]==\"12\")\n",
    "\n",
    "d5a = d5.filter((d5[\"week\"]!=1) | (d5[\"Month\"]!=\"12\")).groupBy(\"Year\",\"week\").agg({\"cancelled\":\"sum\",\"weather\":\"sum\"})\n",
    "d5a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----+\n",
      "|Year|Month|DayofMonth|cancelled|weather|week|\n",
      "+----+-----+----------+---------+-------+----+\n",
      "|2003|   12|        29|      184|     43|   1|\n",
      "|2003|   12|        30|      132|      6|   1|\n",
      "|2003|   12|        31|       93|     18|   1|\n",
      "+----+-----+----------+---------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newYearFirstWeek.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the percentage of then sort them to better visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------------------+\n",
      "|Year|week|         percentage|\n",
      "+----+----+-------------------+\n",
      "|2003|  22| 0.1095890410958904|\n",
      "|2003|  23|0.14702308626974483|\n",
      "|2003|  24|0.29900332225913623|\n",
      "|2003|  25|0.13568985176738882|\n",
      "|2003|  26|0.10589812332439678|\n",
      "|2003|  27| 0.3278837420526794|\n",
      "|2003|  28| 0.3020285499624343|\n",
      "|2003|  29|  0.195852534562212|\n",
      "|2003|  30| 0.3480414227825304|\n",
      "|2003|  31| 0.2532659081331648|\n",
      "|2003|  32|0.20495767835550183|\n",
      "|2003|  33|0.09959211420802175|\n",
      "|2003|  34|0.06289308176100629|\n",
      "|2003|  35|0.26324786324786326|\n",
      "|2003|  36|0.09827586206896552|\n",
      "|2003|  37|0.13633310946944258|\n",
      "|2003|  38| 0.6686946902654868|\n",
      "|2003|  39| 0.1607773851590106|\n",
      "|2003|  40| 0.1203599550056243|\n",
      "|2003|  41|               0.25|\n",
      "+----+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d6 = d5a.withColumn(\"percentage\",d5a[\"sum(weather)\"]/d5a[\"sum(cancelled)\"]).drop(\"sum(weather)\",\"sum(cancelled)\")\\\n",
    "    .sort(\"Year\",\"week\")\n",
    "d6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check what are the years that have the needed informations to compute the query result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994  :  [Row(CancellationCode='NA')]\n",
      "1995  :  [Row(CancellationCode='NA')]\n",
      "1996  :  [Row(CancellationCode='NA')]\n",
      "1997  :  [Row(CancellationCode='NA')]\n",
      "1998  :  [Row(CancellationCode='NA')]\n",
      "1999  :  [Row(CancellationCode='NA')]\n",
      "2000  :  [Row(CancellationCode='NA')]\n",
      "2001  :  [Row(CancellationCode='NA')]\n",
      "2002  :  [Row(CancellationCode='NA')]\n",
      "2003  :  [Row(CancellationCode='NA'), Row(CancellationCode=None), Row(CancellationCode='B'), Row(CancellationCode='D'), Row(CancellationCode='C'), Row(CancellationCode='A')]\n",
      "2004  :  [Row(CancellationCode=None), Row(CancellationCode='B'), Row(CancellationCode='D'), Row(CancellationCode='C'), Row(CancellationCode='A')]\n",
      "2005  :  [Row(CancellationCode=None), Row(CancellationCode='B'), Row(CancellationCode='D'), Row(CancellationCode='C'), Row(CancellationCode='A')]\n",
      "2006  :  [Row(CancellationCode=None), Row(CancellationCode='B'), Row(CancellationCode='D'), Row(CancellationCode='C'), Row(CancellationCode='A')]\n",
      "2007  :  [Row(CancellationCode=None), Row(CancellationCode='B'), Row(CancellationCode='D'), Row(CancellationCode='C'), Row(CancellationCode='A')]\n",
      "2008  :  [Row(CancellationCode=None), Row(CancellationCode='B'), Row(CancellationCode='D'), Row(CancellationCode='C'), Row(CancellationCode='A')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1994,2009):\n",
    "    d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "        .load('BDdata/'+str(i)+'.csv')\n",
    "    print(i,\" : \",d.select(d[\"CancellationCode\"]).distinct().rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = udf(lambda *x : datetime(x[0],x[1],x[2]).isocalendar()[1], IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getWeatherPercentage(year):\n",
    "    d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "        .load('BDdata/'+year+'.csv')\n",
    "    dr = d.filter(d[\"Cancelled\"]=='1')\\\n",
    "        .select(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"],d[\"CancellationCode\"])\n",
    "\n",
    "    d_cancelled_weather = dr.filter(d[\"CancellationCode\"]=='B')\n",
    "\n",
    "    d2 = dr.groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"])\\\n",
    "        .count()\\\n",
    "        .withColumnRenamed(\"count\",\"cancelled\")\n",
    "    \n",
    "    d3 = d_cancelled_weather.groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"])\\\n",
    "        .count()\\\n",
    "        .withColumnRenamed(\"count\",\"weather\")\n",
    "\n",
    "    d3.orderBy(d2[\"Year\"],d2[\"Month\"],d2[\"DayofMonth\"]).show()\n",
    "    \n",
    "    d4 = d2.join(d3, on=[d2[\"Year\"]==d3[\"Year\"],d2[\"Month\"]==d3[\"Month\"],d2[\"DayofMonth\"]==d3[\"DayofMonth\"]])\\\n",
    "        .select(d2[\"Year\"],d2[\"Month\"],d2[\"DayofMonth\"],d2[\"cancelled\"],d3[\"weather\"])\\\n",
    "        .orderBy(d2[\"Year\"],d2[\"Month\"],d2[\"DayofMonth\"])\n",
    "        \n",
    "    d5 = d4.withColumn(\"week\", week(d4[\"Year\"],d4[\"Month\"],d4[\"DayofMonth\"]))\n",
    "    \n",
    "    newYearFirstWeek = d5.filter(d5[\"week\"]==1).filter(d5[\"Month\"]==\"12\")\n",
    "    \n",
    "    d5a = d5.filter((d5[\"week\"]!=1) | (d5[\"Month\"]!=\"12\")).groupBy(\"Year\",\"week\")\\\n",
    "        .agg({\"cancelled\":\"sum\",\"weather\":\"sum\"})\n",
    "    d6 = d5a.withColumn(\"percentage\",d5a[\"sum(weather)\"]/d5a[\"sum(cancelled)\"])\\\n",
    "        .drop(\"sum(weather)\",\"sum(cancelled)\")\\\n",
    "        .sort(\"Year\",\"week\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly percentages of delays that are due to weather, throughout the entire data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+--------+------------+\n",
      "|Year|Month|DayofMonth|ArrDelay|WeatherDelay|\n",
      "+----+-----+----------+--------+------------+\n",
      "|2003|    6|        29|      26|          26|\n",
      "|2003|    6|         1|       3|           0|\n",
      "|2003|    6|         2|      14|           0|\n",
      "|2003|    6|        10|       0|           0|\n",
      "|2003|    6|         6|       4|           0|\n",
      "|2003|    6|         7|       0|           0|\n",
      "|2003|    6|         8|      17|           0|\n",
      "|2003|    6|         9|       6|           0|\n",
      "|2003|    6|        10|       1|           0|\n",
      "|2003|    6|        11|       5|           0|\n",
      "|2003|    6|        12|      19|           0|\n",
      "|2003|    6|        13|      24|           0|\n",
      "|2003|    6|        17|      11|           0|\n",
      "|2003|    6|        20|      12|           0|\n",
      "|2003|    6|        21|       0|           0|\n",
      "|2003|    6|        22|       7|           0|\n",
      "|2003|    6|        23|      17|           0|\n",
      "|2003|    6|        27|      21|           0|\n",
      "|2003|    6|        28|       2|           0|\n",
      "|2003|    6|         1|       7|           0|\n",
      "+----+-----+----------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "    .load('BDdata/2003.csv')\n",
    "d1 = d.where(\"ArrDelay >= 0\").select(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"],d[\"ArrDelay\"],d[\"WeatherDelay\"])\\\n",
    "    .where(\"WeatherDelay != 'NA'\")\n",
    "d1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----------------+-------------+\n",
      "|Year|Month|DayofMonth|sum(WeatherDelay)|sum(ArrDelay)|\n",
      "+----+-----+----------+-----------------+-------------+\n",
      "|2003|    7|        29|           8979.0|     137186.0|\n",
      "|2003|   12|         5|          40077.0|     501010.0|\n",
      "|2003|   11|         7|           2961.0|     155884.0|\n",
      "|2003|    7|        17|          13679.0|     203466.0|\n",
      "|2003|   10|        25|           5802.0|      83849.0|\n",
      "|2003|    6|        30|           6734.0|     140096.0|\n",
      "|2003|    6|        15|           3589.0|     113804.0|\n",
      "|2003|   11|         5|          22131.0|     285687.0|\n",
      "|2003|    6|         9|           4347.0|     113139.0|\n",
      "|2003|    8|         9|          17343.0|     188613.0|\n",
      "|2003|    8|        15|           5238.0|     209983.0|\n",
      "|2003|    7|        10|          32711.0|     333060.0|\n",
      "|2003|    6|        18|          11921.0|     225328.0|\n",
      "|2003|   10|         1|           1210.0|     112603.0|\n",
      "|2003|   10|        17|           1378.0|     125793.0|\n",
      "|2003|   12|        28|           3039.0|     188629.0|\n",
      "|2003|    9|        13|           1116.0|      57627.0|\n",
      "|2003|    9|        11|          41778.0|     254406.0|\n",
      "|2003|   10|        28|           1264.0|     102593.0|\n",
      "|2003|   10|         7|           2063.0|      84088.0|\n",
      "+----+-----+----------+-----------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d3 = d1.groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]).agg({\"ArrDelay\":\"sum\",\"WeatherDelay\":\"sum\"})\n",
    "d3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----------------+-------------+----+\n",
      "|Year|Month|DayofMonth|sum(WeatherDelay)|sum(ArrDelay)|week|\n",
      "+----+-----+----------+-----------------+-------------+----+\n",
      "|2003|    7|        29|           8979.0|     137186.0|  31|\n",
      "|2003|   12|         5|          40077.0|     501010.0|  49|\n",
      "|2003|   11|         7|           2961.0|     155884.0|  45|\n",
      "|2003|    7|        17|          13679.0|     203466.0|  29|\n",
      "|2003|   10|        25|           5802.0|      83849.0|  43|\n",
      "|2003|    6|        30|           6734.0|     140096.0|  27|\n",
      "|2003|    6|        15|           3589.0|     113804.0|  24|\n",
      "|2003|   11|         5|          22131.0|     285687.0|  45|\n",
      "|2003|    6|         9|           4347.0|     113139.0|  24|\n",
      "|2003|    8|         9|          17343.0|     188613.0|  32|\n",
      "|2003|    8|        15|           5238.0|     209983.0|  33|\n",
      "|2003|    7|        10|          32711.0|     333060.0|  28|\n",
      "|2003|    6|        18|          11921.0|     225328.0|  25|\n",
      "|2003|   10|         1|           1210.0|     112603.0|  40|\n",
      "|2003|   10|        17|           1378.0|     125793.0|  42|\n",
      "|2003|   12|        28|           3039.0|     188629.0|  52|\n",
      "|2003|    9|        13|           1116.0|      57627.0|  37|\n",
      "|2003|    9|        11|          41778.0|     254406.0|  37|\n",
      "|2003|   10|        28|           1264.0|     102593.0|  44|\n",
      "|2003|   10|         7|           2063.0|      84088.0|  41|\n",
      "+----+-----+----------+-----------------+-------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d4 = d3.withColumn(\"week\", week(d3[\"Year\"],d3[\"Month\"],d3[\"DayofMonth\"]))\n",
    "d4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-----------------+-------------+----+\n",
      "|Year|Month|DayofMonth|sum(WeatherDelay)|sum(ArrDelay)|week|\n",
      "+----+-----+----------+-----------------+-------------+----+\n",
      "|2003|   12|        31|           2829.0|      90979.0|   1|\n",
      "|2003|   12|        30|           5751.0|     209404.0|   1|\n",
      "|2003|   12|        29|           8638.0|     249379.0|   1|\n",
      "+----+-----+----------+-----------------+-------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d4.where(\"week = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------------------+------------------+\n",
      "|Year|week|sum(sum(WeatherDelay))|sum(sum(ArrDelay))|\n",
      "+----+----+----------------------+------------------+\n",
      "|2003|  50|               98311.0|         1672254.0|\n",
      "|2003|  32|               90932.0|         1580956.0|\n",
      "|2003|  35|              108086.0|         1329545.0|\n",
      "|2003|  46|               52927.0|         1336317.0|\n",
      "|2003|  40|               14404.0|          624630.0|\n",
      "|2003|  49|              104118.0|         1624586.0|\n",
      "|2003|  22|                2266.0|          104139.0|\n",
      "|2003|  25|               69910.0|         1258701.0|\n",
      "|2003|  28|              128860.0|         1956895.0|\n",
      "|2003|  26|               28127.0|         1058286.0|\n",
      "|2003|  37|               58503.0|         1000855.0|\n",
      "|2003|  51|               42295.0|         1629182.0|\n",
      "|2003|  33|               90469.0|         1611533.0|\n",
      "|2003|  36|               31188.0|          883154.0|\n",
      "|2003|  27|               73370.0|         1076466.0|\n",
      "|2003|  23|               51551.0|          948361.0|\n",
      "|2003|  47|              100140.0|         1500560.0|\n",
      "|2003|  44|               23344.0|         1089923.0|\n",
      "|2003|  29|               48215.0|         1218245.0|\n",
      "|2003|  41|               33305.0|          782708.0|\n",
      "+----+----+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newYearFirstWeek = d4.filter(d4[\"week\"]==1).filter(d4[\"Month\"]==\"12\")\n",
    "d5a = d4.filter((d4[\"week\"]!=1) | (d4[\"Month\"]!=\"12\")).groupBy(\"Year\",\"week\")\\\n",
    "        .agg({\"sum(WeatherDelay)\":\"sum\",\"sum(ArrDelay)\":\"sum\"})\n",
    "d5a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------------------+------------------+\n",
      "|Year|week|sum(sum(WeatherDelay))|sum(sum(ArrDelay))|\n",
      "+----+----+----------------------+------------------+\n",
      "+----+----+----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d5a.where(\"week = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+\n",
      "|Year|week|          percentage|\n",
      "+----+----+--------------------+\n",
      "|2003|  22|0.021759379291139726|\n",
      "|2003|  23| 0.05435799236788522|\n",
      "|2003|  24| 0.08422798224375042|\n",
      "|2003|  25|0.055541387509821634|\n",
      "|2003|  26|0.026577881593444493|\n",
      "|2003|  27| 0.06815821400768812|\n",
      "|2003|  28| 0.06584921521083144|\n",
      "|2003|  29| 0.03957742490221589|\n",
      "|2003|  30| 0.07160054816598709|\n",
      "|2003|  31| 0.07311418046852729|\n",
      "|2003|  32| 0.05751709725001834|\n",
      "|2003|  33|0.056138471877398725|\n",
      "|2003|  34|0.060240938882279056|\n",
      "|2003|  35| 0.08129548078477976|\n",
      "|2003|  36| 0.03531433928850461|\n",
      "|2003|  37| 0.05845302266562089|\n",
      "|2003|  38|0.036452499395229015|\n",
      "|2003|  39|   0.036826458565589|\n",
      "|2003|  40|0.023060051550517907|\n",
      "|2003|  41| 0.04255098964109221|\n",
      "+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " d6 = d5a.withColumn(\"percentage\",d5a[\"sum(sum(WeatherDelay))\"]/d5a[\"sum(sum(ArrDelay))\"])\\\n",
    "        .drop(\"sum(sum(WeatherDelay))\",\"sum(sum(ArrDelay))\")\\\n",
    "        .sort(\"Year\",\"week\")\n",
    "    \n",
    "d6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+--------------------+\n",
      "|Year|week|          percentage|\n",
      "+----+----+--------------------+\n",
      "|2005|   1| 0.06666523925107397|\n",
      "|2005|   2|0.061525768148909366|\n",
      "|2005|   3| 0.09819828459897512|\n",
      "|2005|   4| 0.07318699654293981|\n",
      "|2005|   5|0.059825071738306534|\n",
      "|2005|   6|0.042730355121855995|\n",
      "|2005|   7|   0.042275550868367|\n",
      "|2005|   8|  0.0526360788579327|\n",
      "|2005|   9| 0.04082060164623675|\n",
      "|2005|  10| 0.05587787669915344|\n",
      "|2005|  11| 0.04586946314758946|\n",
      "|2005|  12|0.045018706367879045|\n",
      "|2005|  13| 0.05549382003485314|\n",
      "|2005|  14| 0.03788709568855268|\n",
      "|2005|  15|  0.0315842412528014|\n",
      "|2005|  16| 0.05604493024466552|\n",
      "|2005|  17| 0.05655818168088802|\n",
      "|2005|  18| 0.04442350982424638|\n",
      "|2005|  19| 0.04078598684307204|\n",
      "|2005|  20| 0.05272002108407274|\n",
      "+----+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getWeatherDelayPercentage(year):\n",
    "    d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "        .load('BDdata/'+year+'.csv')\n",
    "\n",
    "    d1 = d.where(\"ArrDelay >= 0\").select(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"],d[\"ArrDelay\"],d[\"WeatherDelay\"])\n",
    "    if d1.where(\"WeatherDelay == 'NA'\").count() > 0:\n",
    "        d1= d1.where(\"WeatherDelay != 'NA'\")\n",
    "\n",
    "    d3 = d1.groupBy(d[\"Year\"],d[\"Month\"],d[\"DayofMonth\"]).agg({\"ArrDelay\":\"sum\",\"WeatherDelay\":\"sum\"})\n",
    "    \n",
    "    week = udf(lambda *x : datetime(x[0],x[1],x[2]).isocalendar()[1], IntegerType())\n",
    "\n",
    "    d4 = d3.withColumn(\"week\", week(d3[\"Year\"],d3[\"Month\"],d3[\"DayofMonth\"]))\n",
    "\n",
    "    newYearFirstWeek = d4.filter(d4[\"week\"]==1).filter(d4[\"Month\"]==\"12\")\n",
    "    \n",
    "    d5a = d4.filter((d4[\"week\"]!=1) | (d4[\"Month\"]!=\"12\")).groupBy(\"Year\",\"week\")\\\n",
    "            .agg({\"sum(WeatherDelay)\":\"sum\",\"sum(ArrDelay)\":\"sum\"})\n",
    "\n",
    "    d6 = d5a.withColumn(\"percentage\",d5a[\"sum(sum(WeatherDelay))\"]/d5a[\"sum(sum(ArrDelay))\"])\\\n",
    "            .drop(\"sum(sum(WeatherDelay))\",\"sum(sum(ArrDelay))\")\\\n",
    "            .sort(\"Year\",\"week\")\n",
    "    \n",
    "    return d6\n",
    "\n",
    "getWeatherDelayPercentage(\"2005\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The percentage of flights belonging to a given \"distance group\" that were able to halve their departure delays by the time they arrived at their destinations. \n",
    "\n",
    "Distance groups assort flights by their total distance in miles. Flights with distances that are less than 200 miles belong in group 1, flights with distances that are between 200 and 399 miles belong in group 2, flights with distances that are between 400 and 599 miles belong in group 3, and so on. The last group contains flights whose distances are between 2400 and 2599 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|DistanceGroup|count(1)|\n",
      "+-------------+--------+\n",
      "|           12|   33528|\n",
      "|           22|     592|\n",
      "|            1|  198627|\n",
      "|           13|   50040|\n",
      "|            6|  200053|\n",
      "|           16|     134|\n",
      "|            3|  461810|\n",
      "|           20|    1079|\n",
      "|            5|  292713|\n",
      "|           19|     668|\n",
      "|           15|    1934|\n",
      "|            9|   70000|\n",
      "|           17|     504|\n",
      "|            4|  340342|\n",
      "|            8|  101648|\n",
      "|           23|     651|\n",
      "|            7|   85147|\n",
      "|           10|   52966|\n",
      "|           25|     274|\n",
      "|           21|     443|\n",
      "+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true')\\\n",
    "        .load('BDdata/2005.csv')\n",
    "d1 = d.where(d[\"Cancelled\"]==0).select(d[\"DepDelay\"],d[\"ArrDelay\"],d[\"Distance\"])\n",
    "d2 = d1.where(d[\"DepDelay\"]>0)\n",
    "distanceGroup = udf(lambda x : x//200+1,IntegerType())\n",
    "d3 = d2.filter(d2[\"DepDelay\"]/d2[\"ArrDelay\"]>=2).withColumn(\"DistanceGroup\",distanceGroup(d2[\"Distance\"]))\\\n",
    "    .drop(d2[\"Distance\"])\n",
    "d4 = d3.groupBy(d3.DistanceGroup).agg({\"*\":\"count\"})\n",
    "d5 = d2.withColumn(\"DistanceGroup\",distanceGroup(d2[\"Distance\"]))\\\n",
    "    .drop(d2[\"Distance\"])\n",
    "d6 = d5.groupBy(d5.DistanceGroup).agg({\"*\":\"count\"})\n",
    "d6.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|DistanceGroup|         percentage|\n",
      "+-------------+-------------------+\n",
      "|            1|0.08827601484188957|\n",
      "|            2|0.09693922686977234|\n",
      "|            3|0.09701175808232823|\n",
      "|            4| 0.1027672165057501|\n",
      "|            5|0.10443335280633248|\n",
      "|            6|0.10570198897292217|\n",
      "|            7| 0.1098923038979647|\n",
      "|            8|0.10642609790650087|\n",
      "|            9|0.10832857142857143|\n",
      "|           10|0.10873012876184722|\n",
      "|           11|0.10228890592487132|\n",
      "|           12| 0.1111608208064901|\n",
      "|           13|0.09840127897681855|\n",
      "|           14|0.10471845092365903|\n",
      "|           15|0.10237849017580145|\n",
      "|           16| 0.1044776119402985|\n",
      "|           17|0.08134920634920635|\n",
      "|           18|0.04950495049504951|\n",
      "|           19|0.08982035928143713|\n",
      "|           20|0.10843373493975904|\n",
      "+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d7 = d4.join(d6,on=[d4.DistanceGroup==d6.DistanceGroup])\\\n",
    "    .withColumn(\"percentage\", d4[\"count(1)\"]/d6[\"count(1)\"]).drop(\"count(1)\")\n",
    "d8 = d7.select(d4.DistanceGroup,d7.percentage).sort(\"DistanceGroup\")\n",
    "d8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A weekly \"penalty\" score for each airport that depends on both the its incoming and outgoing flights. The score adds 0.5 for each incoming flight that is more than 15 minutes late, and 1 for each outgoing flight that is more than 15 minutes late."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
